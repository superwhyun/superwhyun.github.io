---
layout: post
tags: [airflow, python, cron]
title: Airflow
---

cron의 업그레이드 대체제 정도로 보면 된다. 업그레이드니 당연히 여러 기능들이 들어 있으나, 어차피 하는 일의 속성으로 봐서는 cron 의 연장으로 보면 될 듯하다.

## Install

python3가 잘 설치되어 있다는 가정하에 아래의 명령으로 apache-airflow를 설치하고 데이터베이스를 초기화 한다. 다만, 설치하기 이전에 AIRFLOW_HOME을 환경변수로 잡아주면 좋다. 없을 경우 ~/airflow 디렉토리를 자동으로 생성하고 그 곳에 각종 config, log 폴더 등이 생성된다. 이 곳의 dag 디렉토리에 원하는 job flow를 python으로 작성해서 올려두면 airflow scheduler가 동작을 수행하게 된다.
```
export AIRFLOW_HOME=/home/[user name]/airflow/
```
그냥 디폴트(~/airflow)로 쓰고자 할 경우에는 안 해 줘도 무방하다. 
아무튼, 아래와 같이 해서 설치해 보자.
```
pip install apache-airflow
airflow initdb
airflow version
```
airflow 웹 서버와 스케쥴러를 띄워보자. 스케쥴러를 띄우지 않으면 task가 동작하지 않으므로 둘 다 띄워주자. 두 개의 별도 터미널에 각기 띄워 로그를 보면서 해도 좋다. 물론, 한 터미널에 둘 다 띄워도 무방(백그라운드 연산자 포함해서..)하다.
```
airflow webserver&
airflow scheduler&
```
8080 포트를 listening 하고 있는 것을 알 수 있다. 웹 브라우저로 접근해 보면 아래와 같은 화면이 나온다. (물론 이 port는 config 파일에서 수정 가능하다)
![web](/assets/images/2020-02-24-23-26-44.png)

아직 내가 만든 코드는 없기 때문에 example류들만 보인다. 얘네를 안 보이게 하려면 
```
airflow restdb
```
해주면 된다. 다만, 아직은 배우는 중이니 잠시 참아보자.


제일 밑의 tutorial을 선택하고, Run을 하면 오른쪽에 로그가 찍힌다.
좀더 상세한 내용을 보려면 제목을 클릭한다.
![screen](/assets/images/2020-02-24-23-27-13.png)
이 튜토리얼은 그냥 시간을 찍어주는 녀석인데, 어디에 찍는지 모르겠넹..


## Simple example

```
from airflow import DAG
from airflow.operators.python_operator import PythonOperator
from airflow.operators.bash_operator import BashOperator
```

## More Dive


- Q. 각 task의 성공, 실패여부를 어떻게 판단하나? 별도의 프로세스를 분기시킨 경우에는 이를 알 방법이 없을 것 같은데?